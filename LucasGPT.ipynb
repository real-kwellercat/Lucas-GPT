{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO9EH/sfSzd3I8Xk8l0JIGb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/real-kwellercat/Lucas-GPT/blob/main/LucasGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LucasGPT\n",
        "Wanted to talk to a smart person but don't have friends? This is the perfect solution!\n",
        "\n",
        "also this is _literally dialogpt_"
      ],
      "metadata": {
        "id": "ibLsxTwLvw2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# raw credits\n",
        "@misc{peng2022godel,\n",
        "author = {Peng, Baolin and Galley, Michel and He, Pengcheng and Brockett, Chris and Liden, Lars and Nouri, Elnaz and Yu, Zhou and Dolan, Bill and Gao, Jianfeng},\n",
        "title = {GODEL: Large-Scale Pre-training for Goal-Directed Dialog},\n",
        "howpublished = {arXiv},\n",
        "year = {2022},\n",
        "month = {June},\n",
        "url = {https://www.microsoft.com/en-us/research/publication/godel-large-scale-pre-training-for-goal-directed-dialog/},\n",
        "}\n"
      ],
      "metadata": {
        "id": "XoiwPnun0a1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup LucasGPT Dependancies\n",
        "!pip install transformers\n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n"
      ],
      "metadata": {
        "id": "dtkKFCAf0C-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LucasGPT Chat\n",
        "#@markdown _The messages before the \"\">> User:\" are loading messages._ <br> Also, \"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set padding_side='left' when initializing the tokenizer.\" is an error i dont know how to fix. <br> Ignore those please.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "# Let's chat FOREVER UNTIL QUIT!\n",
        "while True:\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(\"LucasGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "\n"
      ],
      "metadata": {
        "id": "3hWUG9yDzhRw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}